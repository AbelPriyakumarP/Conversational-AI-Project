{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57556a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from faiss-cpu) (2.2.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a2f057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b600324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\rosha\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rosha\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rosha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rosha\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "293d704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline, TFTrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d911b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "GROQ_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28ef4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"jobs_1.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82466de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c171467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "204b73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2119ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3fe8383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loraconfig = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"attention.q_lin\", \"attention.k_lin\", \"attention.v_lin\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "model = get_peft_model(model, loraconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "326a7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 599.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Sample data for training\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"The company is looking for a skilled AI Engineer.\",\n",
    "        \"We need a Product Manager with experience in agile methodologies.\",\n",
    "        \"Looking for a Data Scientist to analyze large datasets.\",\n",
    "        \"Hiring a Cybersecurity Specialist to secure our systems.\",\n",
    "        \"Join us as a Full Stack Developer to build scalable web applications.\"\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 1, 0]  \n",
    "}\n",
    "\n",
    "# Create a Dataset object\n",
    "train_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a1c64c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosha\\AppData\\Local\\Temp\\ipykernel_29484\\3694293165.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "c:\\Users\\rosha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.6897557576497396, metrics={'train_runtime': 13.2126, 'train_samples_per_second': 1.135, 'train_steps_per_second': 0.227, 'total_flos': 2034680647680.0, 'train_loss': 0.6897557576497396, 'epoch': 3.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,  \n",
    "    eval_dataset=None,  \n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a65a722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_model\\\\tokenizer_config.json',\n",
       " './lora_model\\\\special_tokens_map.json',\n",
       " './lora_model\\\\vocab.txt',\n",
       " './lora_model\\\\added_tokens.json',\n",
       " './lora_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./lora_model\")\n",
    "tokenizer.save_pretrained(\"./lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "39362ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = GROQ_API_KEY,\n",
    "    model_name = \"llama-3.3-70b-versatile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55d026a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI assistant helping users understand job opportunities based on a provided document. Use the following context from the document to answer the question concisely and professionally. If the question is not directly answered by the document, use your general knowledge but indicate any assumptions.\n",
    "\n",
    "Conversation History:\n",
    "{chat_history}\n",
    "\n",
    "Document Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a6f2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd274797",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=groq_llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58e82469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    response = qa_chain({\"question\": question})\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8d6d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Can you summarize the job title and company name from the document?\",\n",
    "    \"What are the hiring steps for positions listed in the document?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84d4d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you summarize the job title and company name from the document?\n",
      "Answer: The document does not mention specific company names. However, it does list the following job titles:\n",
      "\n",
      "1. Talent Acquisition Associate\n",
      "2. Sustainability Analyst\n",
      "3. Sales Development Representative\n",
      "4. Customer Success Analyst\n",
      "5. Sustainability Specialist\n",
      "6. Customer Success Associate\n",
      "7. Growth Marketing Manager\n",
      "8. Talent Acquisition Partner\n",
      "9. Sustainability Manager\n",
      "10. Workplace Coordinator\n",
      "\n",
      "Additionally, the document mentions other job titles from various industries, including:\n",
      "\n",
      "* Trades Workers\n",
      "* Teaching Professionals\n",
      "* Information and Communications Technology Professionals\n",
      "* Science and Engineering Associate Professionals\n",
      "* Personal Services Workers\n",
      "* Labourers\n",
      "* Science and Engineering Professionals\n",
      "* Personal Care Workers\n",
      "* Sales Workers\n",
      "* Legal, Social, and Cultural Professionals\n",
      "* Food Preparation Assistants\n",
      "* Bricklayers and Related Workers\n",
      "* Metal Working Machine Tool Setters and Operators\n",
      "* Sheet Metal Workers\n",
      "* Motor Vehicle Mechanics and Repairers\n",
      "* Nursing Professionals\n",
      "* Specialist Medical Practitioners\n",
      "* Physiotherapists\n",
      "* Software Developers\n",
      "* Applications Developers\n",
      "\n",
      "Question: What are the hiring steps for positions listed in the document?\n",
      "Answer: I don't know the specific hiring steps for each of the job positions you've listed. The provided context only discusses the growth of certain job roles and industries, such as sales, sustainability, and talent acquisition, but does not outline the hiring steps for these positions. \n",
      "\n",
      "If you're looking for information on hiring steps, I recommend checking the specific company's website, job descriptions, or contacting their HR department for more detailed information on their hiring process. Additionally, you can also search for general guidelines and best practices for hiring in specific industries, but it's essential to note that hiring steps can vary significantly from company to company.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in questions:\n",
    "    print(f\"Question: {q}\")\n",
    "    print(f\"Answer: {ask_question(q)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b796467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
